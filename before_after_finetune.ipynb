{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44468bc42bd1425890fb1919c80f7d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "I have seen a lot of good reviews for this show. I've never seen it but I will try to find it.\n",
      "I'm not a huge TV watcher, but I really enjoyed \"The Wire\" and \"The Sopranos.\"\n",
      "\"The Sopranos\" was amazing. I watched it in a couple of weeks. I was sad when it ended.\n",
      "I have heard of both of those shows, but I haven't seen them.\n",
      "I'm a huge fan of \"The Wire\". I also loved \"Breaking Bad\" and \"The Sopranos\".\n",
      "I watched \"The Wire\" and \"Breaking Bad\" and both were awesome. I am not a huge tv watcher but I am definitely going to check out \"The Sopranos\".\n"
     ]
    }
   ],
   "source": [
    "# load baseline model no quantization\n",
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='cuda')\n",
    "\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode('I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n', return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_length=200, do_sample=True,)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "# 24074MB (2.5 min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463ac3bc5229448c8f431d3a8caba2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      " Unterscheidung zwischen \"good\" und \"great\" ist subjektiv.\n",
      " I've read that \"The Wire\" is a great show, but I haven't seen it yet.\n",
      " I'm also interested in your recommendations of good movies.\n",
      " I'm a big fan of \"The Wire\". It's a great show.\n",
      " I'm not sure if I'd say \"Breaking Bad\" is the best show ever, but it's certainly great.\n",
      " I'm not sure if I'd say \"Breaking Bad\" is the best show ever, but it's certainly a good show.\n",
      " I'm not sure if I'd say \"Breaking Bad\" is the best show ever, but it's certainly a great show.\n",
      " I'm not\n"
     ]
    }
   ],
   "source": [
    "# load baseline model with quantization\n",
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map='sequential')\n",
    "\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode('I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n', return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(inputs, max_length=200, do_sample=True,)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "# 5905MB (22s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22015eaf91f94e36a1297810d55abf69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      " hopefully someone can help me.\n",
      "I'm a big fan of \"The Wire\" and \"The Shield\".\n",
      "\"The Wire\" is one of the best shows ever. I'm re-watching it right now.\n",
      "I've heard good things about \"The Wire\" and \"The Shield\", but I've never seen them. I'll have to check them out.\n",
      "I've heard good things about \"The Wire\" and \"The Shield\", but I've never seen them.\n",
      "The Wire is one of the best shows ever. I'm re-watching it right now.\n",
      "The Wire is one of the best shows ever. I'm re-watching it right now. I'm a big fan of \"The Wire\"\n"
     ]
    }
   ],
   "source": [
    "# load merged model with quantization\n",
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"/home/rapids/mounted/finetune_llm/results/llama2/final_merged_checkpoint\" # for some reason this must be full path instead of relative\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map='sequential')\n",
    "\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode('I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n', return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(inputs, max_length=200, do_sample=True,)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000ecefdf51d4e5f9c7308ca4e84111a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      " Hinweis: Es ist empfehlenswert, dass Sie ein Video-Streaming-Abonnement abonnieren.\n",
      "The best show I've seen in the last 10 years is \"Better Call Saul\"\n",
      "There are so many great shows that I'm not sure where to start.\n",
      "\"Breaking Bad\" is one of the best shows I've seen.\n",
      "\"Band of Brothers\" is a great series.\n",
      "\"The Wire\" is another great series.\n",
      "\"The Sopranos\" is another great series.\n",
      "\"Mad Men\" is another great series.\n",
      "\"Dexter\" is another great series.\n",
      "\"Game of Thrones\" is another great series.\n",
      "\"The Walking Dead\" is another great series.\n",
      "\"Fargo\" is another\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load complex merged model with quantization\n",
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"/home/rapids/mounted/finetune_llm/results/llama2/complex_merged_checkpoint\" # for some reason this must be full path instead of relative\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map='sequential')\n",
    "\n",
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode('I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n', return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(inputs, max_length=200, do_sample=True,)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> Translate the following sentence into Chinese. \"The sky is blue\"\n",
      "Chinese is a tonal language, so the tone you use will change the meaning of a word.\n",
      "Chinese is a tonal language, so the tone you use will change the meaning of a word. In Chinese, the word \"blue\" (蓝) has a different tone than \"sky\" (天).\n",
      "The sky is blue. 天蓝。\n",
      "The sky is blue. 天是蓝的。\n",
      "The sky is blue. 天是蓝的。\n",
      "The sky is blue. 天是蓝的。\n",
      "The sky is blue. 天是蓝的。\n",
      "The sky is blue. 天是蓝的。\n",
      "The sky is blue. 天是蓝的。\n",
      "The sky is blue. 天是蓝的。\n",
      "The sky is blue. \n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.encode('Translate the following sentence into Chinese. \"The sky is blue\"\\n', return_tensors=\"pt\").to('cuda')\n",
    "outputs = model.generate(inputs, max_length=200, do_sample=True,)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare llama 2 before and after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349a27b2f54c4c1fac2d16487cbf4fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "I'm not a fan of \"House of Cards\" but I've heard it's good. I haven't seen \"Better Call Saul\" yet. I'll check that out.\n",
      "I'm not a big TV watcher. I don't have cable and only have Netflix and Amazon Prime. I've been watching \"The Wire\" on Netflix. I'm on season 4 now. I'm enjoying it so far.\n",
      "I've heard of \"The Wire\" but haven't seen it. I'll check it out. I've also heard good things about \"The Sopranos\".\n",
      "I've never seen \"The Sopranos\". I've heard it's good though.\n",
      "I've\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = \"meta-llama/Llama-2-7b-hf\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43395297572c4c2697e19724959b6238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['quantization_config'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m bnb_config \u001b[39m=\u001b[39m create_bnb_config()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m pipeline \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mpipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mI liked \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBreaking Bad\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m and \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBand of Brothers\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m. Do you have any recommendations of other shows I might like?\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     top_k\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m sequences:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResult: \u001b[39m\u001b[39m{\u001b[39;00mseq[\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:205\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    165\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[39miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_single\u001b[39m(\u001b[39mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess(inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocess(model_outputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:268\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m         generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mmin_length\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m prefix_length\n\u001b[1;32m    267\u001b[0m \u001b[39m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    269\u001b[0m out_b \u001b[39m=\u001b[39m generated_sequence\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    270\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1433\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1431\u001b[0m model_kwargs \u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mupdate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# All unused kwargs must be model kwargs\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m generation_config\u001b[39m.\u001b[39mvalidate()\n\u001b[0;32m-> 1433\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m   1435\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m logits_processor \u001b[39m=\u001b[39m logits_processor \u001b[39mif\u001b[39;00m logits_processor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m LogitsProcessorList()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m   1248\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1249\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1250\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1251\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1252\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['quantization_config'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "# model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model = \"meta-llama/Llama-2-7b-hf\"\n",
    "model = \"/home/rapids/mounted/finetune_llm/results/llama2/final_merged_checkpoint\" # for some reason this must be full path instead of relative\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config\n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    quantization_config=bnb_config,\n",
    "    # torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "import os\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, AutoPeftModelForCausalLM\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Trainer, TrainingArguments, BitsAndBytesConfig, \\\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name, bnb_config):\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    max_memory = f'{24564}MB'\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\", # dispatch efficiently the model on the available ressources\n",
    "        max_memory = {i: max_memory for i in range(n_gpus)},\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "    # Needed for LLaMA tokenizer\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bnb_config():\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "    return bnb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c4cd41c6744ecba744e4a9f2b9f9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:648: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"/home/rapids/mounted/finetune_llm/results/llama2/final_merged_checkpoint\" # for some reason this must be full path instead of relative\n",
    "\n",
    "bnb_config = create_bnb_config()\n",
    "\n",
    "model, tokenizer = load_model(model_name, bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      " nobody is perfect and that is why we are here to help each other\n",
      "You have good taste in tv series.\n",
      "\"Breaking Bad\" and \"Band of Brothers\" are among the best shows I've ever seen.\n",
      "I'm not a big tv show watcher. I've seen a few shows that I liked, but nothing that I'd recommend.\n",
      "If you're looking for a good series, I recommend \"The Wire\" and \"Game of Thrones\".\n",
      "I'm a big fan of \"The Wire\".\n",
      "I've never seen \"Game of Thrones\", but I've heard good things about it.\n",
      "I've seen \"The Wire\" and it's great.\n",
      "I've never seen \"Game of Thrones\", but I've heard good things about it.\n",
      "I'm a big fan of \"The Wire\". I've never seen \"Game of Thrones\", but I've heard good things about it.\n",
      "I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it.\n",
      "I'm a big fan of \"The Wire\". I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great.\n",
      "I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it.\n",
      "I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\", but I've heard good things about it. I've seen \"The Wire\" and it's great. I've never seen \"Game of Thrones\n"
     ]
    }
   ],
   "source": [
    "# device = 'cuda'\n",
    "# inputs = tokenizer.encode('I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n', return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m sequences \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mI liked \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBreaking Bad\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBand of Brothers\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m. Do you have any recommendations of other shows I might like?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     do_sample\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     top_k\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     num_return_sequences\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     eos_token_id\u001b[39m=\u001b[39mtokenizer\u001b[39m.\u001b[39meos_token_id,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     max_length\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/gsiis/Documents/mount_src/finetune_llm/before_after_finetune.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
